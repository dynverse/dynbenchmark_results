---
output: dynbenchmark::github_markdown_nested
---

```{r setup, include=FALSE}
library(tidyverse)
library(dynbenchmark)
```

# Metric characterisation and testing

## Specific metrics

### `r label_metric("isomorphic")`, `r label_metric("edge_flip")` and `r label_metric("him")`: Edit distance between two trajectory topologies

We used three different scores to assess the similarity in the topology between two trajectories, iregardless of where the cells were positioned.

For all three scores, we first simplified the topology of the trajectory to make both graph structures comparable:

- As we are only interested in the main structure of the topology without start or end, the graph was made undirected.
- All milestones with degree 2 were removed. For example in the topology A ⇨ B ⇨ C ⇨ D, C ⇨ D, the B milestone was removed
- A linear topology was converted to A ⇨ B ⇨ C
- A cyclical topology such as A ⇨ B ⇨ C ⇨ D or A ⇨ B ⇨ A were all simplified to A ⇨ B ⇨ C ⇨ A
- Duplicated edges such as A ⇨ B, A ⇨ B were decoupled to A ⇨ B, A ⇨ C ⇨ B

The `r label_metric("isomorphic")` score returns 1 if two graphs are isomorphic, and 0 if they were not. For this, we used the used the BLISS algorithm [@junttila_engineeringefficientcanonical_2007], as implemented in the R *igraph* package.

The `r label_metric("edge_flip")` score was defined as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem without a scalable solution [@bahiense_maximumcommonedge_2012]. We implemented a branch and bound approach for this problem, using several heuristics to speed up the search:

- First check all possible edge additions and removals corresponding to the number of different edges between the two graphs.
- For each possible solution, first check whether:
  - The maximal degree is the same
  - The minimal degree is the same
  - All degrees are the same after sorting
- Only then check if the two graphs are isomorphic as described earlier.
- If no solution is found, check all possible solutions with two extra edge additions/removals.

The `r label_metric("him")` metric (Hamming-Ipsen-Mikhailov distance) [@jurman_HIM_Glocal_Metric_2015] which was adopted from the R nettools package ([https://github.com/filosi/nettools](https://github.com/filosi/nettools)). It uses an adjacency matrix which was weighted according to the lengths of each edges within the milestone network. Conceptually, `r label_metric("him")` is a linear combination of:

- The normalised Hamming distance [@doughertyValidationGeneRegulatory2011], which calculates the distance between two graphs by matching individual edges in the adjacency matrix, but disregards overall structural similarity.
- The normalised Ipsen-Mikhailov distance [@ipsenEvolutionaryReconstructionNetworks2002], which calculates the overall distance of two graphs based on matches between its degree and adjacency matrix, while disregarding local structural similarities. It requires a $\gamma$ parameter, which is usually estimated based on the number of nodes in the graph, but which we fixed at $0.1$ so as to make the score comparable across different graph sizes.

We compared the three scores for several common topologies (`r ref("fig", "topology_scores_overview")`). While conceptually very different, the `r label_metric("edge_flip")` and `r label_metric("him")` still produce similar scores (`r ref("fig", "topology_scores_overview", "b")`). The `r label_metric("him")` tends to punish the detection of cycles, while the `r label_metric("edge_flip")` is more harsh for differences in the number of bifurcations (`r ref("fig", "topology_scores_overview", "b")`). The main difference however is that the `r label_metric("him")` takes into account edge lengths when comparing two trajectories, as illustrated in (`r ref("fig", "topology_scores_overview", "c")`). Short "extra" edges in the topology are less punished by the `r label_metric("him")` than by the `r label_metric("edge_flip")`.

```{r}
add_fig(
  result_file("topology_scores_overview.rds", experiment_id = "02-metrics/01-metric_characterisation"),
  "topology_scores_overview",
  glue::glue("Showcase of three metrics to evaluate topologies: {label_metric('isomorphic')}, {label_metric('edge_flip')} and {label_metric('him')}."),
  glue::glue("(a) The used topologies. (b) The scores when comparing each pair of trajectory types. (d) Four datasets in which aan extra edge is added and made progressively longer. This shows how the {label_metric('him')} can take into account edge lengths."),
  width = 12,
  height = 12
)
```

To summarise, the different topology based scores are useful for different scenarios:

- If the two trajectories should only be compared when the topology is exactly the same, the `r label_metric("isomorphic")` should be used.
- If it is important that the topologies are similar, but not necessarily isomorphic, the `r label_metric("edge_flip")` is most appropriate.
- If the topologies should be similar, but shorter edges should not be punished as hard as longer edges, the `r label_metric("HIM")` is most appropriate.

### `r label_metric("F1_branches")` and `r label_metric("F1_milestones")`: Comparing how well the cells are clustered in the trajectory

Perhaps one of the simplest ways to calculate the similarity between the cellular positions of two topologies is by mapping each cell to its closest milestone _or_ branch (`r ref("fig", "clustering_scores_overview")`). These clusters of cells can then be compared using one of the many external cluster evaluation measures [@saelens_comprehensiveevaluationmodule_2018]. When selecting a cluster evaluation metric, we had two main conditions:

- Because we allow methods to filter cells in the trajectory, the metric should be able to handle "non-exhaustive assignment", where some cells are not assigned to any cluster.
- The metric should give each cluster equal weight, so that rare cell stages are equally important as large stages.

The $\textrm{F1}$ score between the $\textrm{Recovery}$ and $\textrm{Relevance}$ is a metric which conforms to both these conditions. This metric will map two clustersets by using their shared members based on the $\textrm{Jaccard}$ similarity. It then calculates the $\textrm{Recovery}$ as the average maximal $\textrm{Jaccard}$ for every cluster in the first set of clusters (in our case the reference trajectory). Conversely, the $\textrm{Relevance}$ is calculated based on the average maximal similarity in the second set of clusters (in our case the prediction). Both the $\textrm{Recovery}$ and $\textrm{Relevance}$ are then given equal weight in a harmonic mean ($\textrm{F1}$). Formally, if $C$ and $C'$ are two cell clusters:

$\textrm{Jaccard}(c, c') = \frac{|c \cap c'|}{|c \cup c'|}$

$\textrm{Recovery} = \frac{1}{|C|} \sum_{c \in C}{\max_{c' \in C'}{\textrm{Jaccard(c, c')}}}$

$\textrm{Relevance} = \frac{1}{|C'|} \sum_{c' \in C'}{\max_{c \in C}{\textrm{Jaccard(c, c')}}}$

$\textrm{F1} = \frac{2}{\frac{1}{\textrm{Recovery}} + \frac{1}{\textrm{Relevance}}}$





```{r}
add_fig(
  result_file("clustering_scores_overview.rds", experiment = "02-metrics/01-metric_characterisation"),
  "clustering_scores_overview",
  glue::glue("Mapping cells to their closest milestone or branch for the calculation of the {label_metric('F1_milestones')} and {label_metric('F1_branches')} ."),
  glue::glue("To calculate the {label_metric('F1_milestones')}, cells are mapped towards the nearest milestone, i.e. the milestone with the highest milestone percentage. For the {label_metric('F1_branches')}, the cells are mapped to the closest edge."),
  width = 12,
  height = 5
)
```

### `r label_metric("correlation")`: Correlation between geodesic distances

When the position of a cell is the same in both the reference and the prediction, its _relative_ distances to all other cells in the trajectory should also be the same. This observation is the basis for the `r label_metric("correlation")` metric.

```{r}
add_fig(
  raw_file("metrics_geodesic.svg", "02-metrics/01-metric_characterisation"),
  "metrics_geodesic",
  "The calculation of geodesic distances on a small example trajectory.",
  "a) A toy example containing four milestones (W to Z) and five cells (a to e). b) The corresponding milestone network, milestone percentages and regions of delayed commitment, when the toy trajectory is converted to the common trajectory model. c) The calculations made for calculating the pairwise geodesic distances. d) A heatmap representation of the pairwise geodesic distances.",
  width = 12,
  height = 6
)
```

The geodesic distance is the distance a cell has to go through the trajectory space to get from one position to another. The way this distance is calculated depends on how two cells are positioned, showcased by an example in `r ref("fig", "metrics_geodesic")`:

- **Both cells are on the same edge in the milestone network.** In this case, the geodesic distance is defined as the product of the difference in milestone percentages and the length of their shared edge. For cells $a$ and $b$ in the example, $d(a, b)$ is equal to $1 \times (0.9 - 0.2) = 0.7$.
- **Cells reside on different edges in the milestone network.** First, the distance of the cell to all its nearby milestones is calculated, based on its percentage within the edge and the length of the edge. These distances in combination with the milestone network are used to calculate the shortest path distance between the two cells. For cells $a$ and $c$ in the example, $d(a, X) = 1 \times 0.9$ and $d(c, X) = 3 \times 0.2$, and therefore $d(a, c) = 1 \times 0.9 + 3 \times 0.2$. 

The geodesic distance can be easily extended towards cells within regions of delayed commitment. When both cells are part of the same region of delayed commitment, the geodesic distance was defined as the manhattan distances between the milestone percentages weighted by the lengths from the milestone network. For cells $d$ and $e$ in the example, $d(d, e)$ is equal to $0 \times (0.3 - 0.2) + 2 \times (0.7 - 0.2) + 3 \times(0.4 - 0.1) = 1.9$. The distance between two cells where only one is part of a region of delayed commitment is calculated similarly to the previous paragraph, by first calculating the distance between the cells and their neighbouring milestones first, then calculating the shortest path distances between the two.

Calculating the pairwise distances between cells scales quadratically with the number of cells, and would therefore not be scaleable for large datasets. For this reason, a set of waypoint cells are defined *a priori*, and only the distances between the waypoint cells and all other cells is calculated, in order to calculate the correlation of geodesic distances of two trajectories (`r ref("fig", "waypoints_overview", "a")`). These cell waypoints are determined by viewing each milestone, edge and region of delayed commitment as a collection of cells. We do stratified sampling from each collection of cells by weighing them by the total number of cells within that collection. For calculating the `r label_metric("correlation")` between two trajectories, the distances between all cells and the union of both waypoint sets is computed.

To select the number of cell waypoints, we need to find a trade-off between the accuracy versus the time to calculate `r label_metric("correlation")`. To select an optimal number of cell waypoints, we used the synthetic dataset with the most complex topology, and determined the `r label_metric("correlation")` at different levels of both cell shuffling and number of cell waypoints (`r ref("fig", "waypoints_overview", "b")`). We found that using cell waypoints does not induce a systematic bias in the `r label_metric("correlation")`, and that its variability was relatively minimal when compared to the variability between different levels of cell shuffling when using 100 or more cell waypoints.

```{r}
add_fig(
  result_file("waypoints_overview.rds", experiment = "02-metrics/01-metric_characterisation"),
  "waypoints_overview",
  glue::glue("Determination of cell waypoints"),
  glue::glue("a) Illustration of the stratified cell sampling using an example dataset (top). Each milestone, edge between two milestones and region of delayed commitment is seen as a collection of cells (middle), and the number of waypoints (100 in this case) are divided over each of these collection of cells (bottom). b) Accuracy versus time to calculate {label_metric('correlation')}"),
  width = 12, height = 8
)
```

Although the `r label_metric("correlation")`'s main characteristic is that it looks at the positions of the cells, other features of the trajectory are also (partly) captured. To illustrate this, we used the geodesic distances themselves as input for dimensionality reduction (`r ref("fig", "geodesic_distances_dimreds")`) with varying topologies. This reduced space captures the original trajectory structure quite well, including the overall topology and branch lengths. Only some structures, not easily visualisabe

```{r}
add_fig(
  result_file("geodesic_distances_dimreds.rds", experiment = "02-metrics/01-metric_characterisation"),
  "geodesic_distances_dimreds",
  glue::glue("The geodesic distances can be used to reconstruct the original trajectory structure"),
  glue::glue("We generated different toy trajectory datasets with varying topologies and calculated the geodesic distances between all cells within the trajectory. We then used these distances as input for classical multidimensional scaling. This shows that the geodesic distances do not only contain information regarding the cell's positions, but also information on the lengths and wiring of the topology."),
  width = 12, height = 8
)
```

### `r label_metric("rf_nmse")` and `r label_metric("lm_nmse")`: Using the positions of the cells within one trajectory to predict the cellular positions in the other trajectory

An alternative approach to detect whether the positions of cells are similar between two trajectories, is to use the positions of one trajectory to predict the positions within the other trajectory. If the cells are at similar positions in the trajectory (relative to its nearby cells), the prediction error should be low.

Specifically, we implemented two metrics which predict the milestone percentages from the reference by using the predicted milestone percentages as features  (`r ref("fig", "metrics_prediction")`). We did this with two regression methods, linear regression ($\textit{lm}$, using the R `lm` function) and Random Forest ($\textit{rf}$, implemented in the *ranger* package [@wright_rangerfastimplementation_2017]). In both cases, the accuracy of the prediction was measured using the Mean Squared error ($\mathit{MSE}$), in the case of Random forest we used the out-of-bag mean-squared error. Next, we calculated $\mathit{MSE}_{worst}$ equal to the $\mathit{MSE}$ when predicting all milestone percentages as the average. We used this to calculate the normalised mean squared error as $\mathit{NMSE} = 1 - \frac{\mathit{MSE}}{\mathit{MSE}_{worst}}$. We created a regression model for every milestone in the gold standard, and averaged the $\mathit{NMSE}$ values to finally obtain the `r label_metric("rf_nmse")` and `r label_metric("lm_nmse")` scores.

```{r}
add_fig(
  raw_file("metrics_prediction.svg", "02-metrics/01-metric_characterisation"),
  "metrics_prediction",
  glue::glue("The calculation of {label_metric('lm_nmse')} distances on a small example trajectory."),
  "The milestone percentages of the reference are predicted based on the milestone percentages of the prediction, using regression models such as linear regression or random forests. The predicted trajectory is then scored by comparing the mean-squared error (MSE° of this regression model with the baseline MSE where the prediction is the average milestone percentage",
  width = 12,
  height = 6
)
```


## Application metrics

Although most metrics described above already assess some aspects directly relevant to the user, such as whether the method is good at finding the right topology, these metrics do not assess the quality of downstream analyses and hypotheses which can be generated from these models. 

### `r label_metric("featureimp_cor")` and `r label_metric("featureimp_wcor")`: The accuracy of dynamical differentially expressed features/genes.

Perhaps the main advantage of studying cellular dynamic processes using single-cell -omics data is that the dynamics of gene expression can be studied for the whole transcriptome. This can be used to construct other models such as dynamic regulatory networks and gene expression modules. Such analyses rely on a "good-enough" cellular ordering, so that it can be used to identify dynamical differentially expressed genes.

To calculate the `r label_metric("featureimp_cor")` we used Random forest regression to rank all the features according to their importance in predicting the positions of cells in the trajectory. More specifically, we first calculated the geodesic distances for each cell to all milestones in the trajectory. Next, we trained a Random Forest regression model (implemented in the R _ranger_ package [@wright_rangerfastimplementation_2017], [https://github.com/imbs-hl/ranger](https://github.com/imbs-hl/ranger)) to predict these distances for each milestone, based on the expression of genes within each cell. We then extracted feature importances using the Mean Decrease in Impurity (`importance = 'impurity'` parameter of the `ranger` function), as illustrated in (`r ref("fig", "featureimp_overview")`). The overall importance of a feature (gene) was then equal to the mean importance over all milestones. Finally, we compared the two rankings by calculating the Pearson correlation, with values between -1 and 0 clipped to 0.

```{r}
add_fig(
  result_file("featureimp_overview.rds", experiment = "02-metrics/01-metric_characterisation"),
  "featureimp_overview",
  glue::glue("An illustration of ranking features based on their importance in a trajectory."),
  glue::glue("(a) A MDS dimensionality reudction of a real dataset in which mouse embryonic fibroblasts (MEF) differentiate into Neurons and Myocytes. (b) The ranking of feature importances from high to low. The majority of features have a very low importance. (c) Some examples, which were also highlighted in b. Higher features in the ranking are clearly specific to certain parts of the trajectory, while features lower on the ranking have a more dispersed expression pattern."),
  width = 12, height = 8
)
```

Random forest regression has two main hyperparameters. The number of trees to be fitted (`num_trees` parameter) was fixed to `10000` to provide accurate and stable estimates of the feature importance (`r ref("fig", "featureimp_cor_distributions")`). The number of features on which can be split (`mtry` parameter) was set to 1% of all available features (instead of the default square-root of the number of features), as to make sure that predictive but highly correlated features, omnipresent in transcriptomics data, are not suppressed in the ranking.

```{r}
add_fig(
  result_file("featureimp_cor_distributions.rds", experiment = "02-metrics/01-metric_characterisation"),
  "featureimp_cor_distributions",
  glue::glue("Effect of the number of trees parameter on the accuracy and variability of the {label_metric('featureimp_cor')}."),
  glue::glue("We used the dataset from {ref('fig', 'featureimp_overview')} and calculated the {label_metric('featureimp_cor')} after shuffling a percentage of cells."),
  width = 12, height = 6
)
```

For most datasets, only a limited number of features will be differentially expressed in the trajectory. For example, in the dataset used in `r ref("fig", "featureimp_cor_distributions")` only the top 10%-20% show a clear pattern of differential expression. The correlation will weight each of these features equally, and will therefore give more weight to the bottom, irrelevant features. To prioritise the top differentially expressed features, we also implemented the `r label_metric("featureimp_wcor")`, which will weight the correlation using the feature importance scores in the reference so that the top features have relatively more impact on the score (`r ref("fig", "featureimp_wcor_effect")`).

```{r}
add_fig(
  result_file("featureimp_wcor_effect.rds", experiment = "02-metrics/01-metric_characterisation"),
  "featureimp_wcor_effect",
  glue::glue("Effect of weighting the features based on their feature importance in the reference."),
  glue::glue("We used the same dataset as in {ref('fig', 'featureimp_overview')}, and calculated the {label_metric('featureimp_cor')} after shuffling a percentage of cells."),
  width = 8, 
  height = 5
)
```
