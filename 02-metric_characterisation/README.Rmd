---
title: "Metrics to compare two trajectories"
output: 
  pdf_document:
    latex_engine: xelatex
    # keep_tex: true
    number_sections: true
header-includes:
  - \usepackage{tabularx}
  - \newcommand{\hideFromPandoc}[1]{#1}
  - \hideFromPandoc{\newenvironment{myfigure}[1]{\begin{figure}[#1]}{\end{figure}}}
mainfont: DejaVu Sans
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE
)
options(knitr.duplicate.label='allow')

library(tidyverse)
library(dynbenchmark)

refs <- setup_refs()
figs <- setup_figs()
tables <- setup_tables()
```

A trajectory, as defined in our evaluation, is a model with multiple abstractions. The top abstraction is the topology which contains information about the paths each cell can take from their starting point. Deeper abstractions involve the mapping of each cell to a particular branch within this network, and the position (or ordering) of each cells within these branches. Internally, the topology is represented by the milestone network and regions of delayed commitment, the branch assignment and cellular positions are represented by the milestone percentages (`r ref("fig", "trajectory_model_example")`).


```{r, results = "asis"}
add_fig(
  result_file("trajectory_model_example.svg", "manual_figures"),
  "trajectory_model_example",
  "An example trajectory that will be used throughout this section.",
  "It contains contains four milestones (W to Z) and five cells (a to e)."
)
```

Given the multilayered complexity of a trajectory model, it is not trivial to compare the similarity of two trajectory models using only one metric. We therefore sought to use different comparison metrics, each serving a different purpose:

- **Specific metrics** investigate one particular aspect of the trajectory. Such metrics make it possible to find particular weak points for methods, e.g. that a method is very good at ordering but does not frequently find the correct topology. Moreover, this makes it possible to create personalised rankings of methods, for example for users which are primarily interested in using the method correct topology.
- **Application metrics** focus on the quality of a downstream analysis using the trajectory. For example, it measures whether the trajectory can be used to find accurate differentially expressed genes.
- **Overall metrics** should capture all the different abstractions, in other words such metrics measure whether the resulting trajectory has a good topology, that the cells belong to similar branches _and_ that they are ordered correctly.

Here, we first described and illustrate several specific, application and overall metrics which we defined. Next, we tested these metrics on several test cases, to make sure they were robustly identify different wrong trajectory predictions. Based on this robustness assessment, we chose 4 metrics for the final evaluation.

All metrics described here were implemented within the [_dyneval_](https://github.com/dynverse/dyneval) R package ([https://github.com/dynverse/dyneval](https://github.com/dynverse/dyneval)).

# Specific metrics

## `r label_metric("isomorphic")`, `r label_metric("edge_flip")` and `r label_metric("him")`: Edit distance between two trajectory topologies

We used three different scores to assess the similarity in the topology between two trajectories, iregardless of where the cells were positioned.

For all three scores, we first simplified the topology of the trajectory to make both graph structures comparable:

- As we are only interested in the main structure of the topology without start or end, the graph was made undirected.
- All milestones with degree 2 were removed. For example in the topology A ⇨ B ⇨ C ⇨ D, C ⇨ D, the B milestone was removed
- A linear topology was converted to A ⇨ B ⇨ C
- A cyclical topology such as A ⇨ B ⇨ C ⇨ D or A ⇨ B ⇨ A were all simplified to A ⇨ B ⇨ C ⇨ A
- Duplicated edges such as A ⇨ B, A ⇨ B were decoupled to A ⇨ B, A ⇨ C ⇨ B

The `r label_metric("isomorphic")` score returns 1 if two graphs are isomorphic, and 0 if they were not. For this, we used the used the BLISS algorithm [@junttila_engineeringefficientcanonical_2007], as implemented in the R *igraph* package.

The `r label_metric("edge_flip")` score was defined as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem without a scalable solution [@bahiense_maximumcommonedge_2012]. We implemented a branch and bound approach for this problem, using several heuristics to speed up the search:

- First check all possible edge additions and removals corresponding to the number of different edges between the two graphs. If no solution is found, check all possible solutions with two extra edge additions/removals.
- For each possible solution, first check whether:
  - The maximal degree is the same
  - The minimal degree is the same
  - All degrees are the same after sorting
- Only then check if the two graphs are isomorphic as described earlier.

The `r label_metric("him")` metric (Hamming-Ipsen-Mikhailov distance) [@jurmanHIMGlocalMetric2015] which was adopted from the R nettools package [https://github.com/filosi/nettools](https://github.com/filosi/nettools). It uses an adjacency matrix which was weighted according to the lengths of each edges within the milestone network. Conceptually, `r label_metric("him")` is a linear combination of:

- The normalised Hamming distance [@doughertyValidationGeneRegulatory2011], which calculates the distance between two graphs by matching individual edges in the adjacency matrix, but disregards overall structural similarity.
- The normalised Ipsen-Mikhailov distance [@ipsenEvolutionaryReconstructionNetworks2002], which calculates the overall distance of two graphs based on matches between its degree and adjacency matrix, while disregarding local structural similarities. It requires a $\gamma$ parameter, which is usually estimated based on the number of nodes in the graph, but which we fixed at $0.1$ so as to make the score comparable across different graph sizes.

We compared the three scores for several common topologies (`r ref("fig", "topology_scores_overview")`). While conceptually very different, the `r label_metric("edge_flip")`r and `r label_metric("him")`r still produce similar scores (`r ref("fig", "topology_scores_overview", "b")`). The `r label_metric("him")`r tends to punish the detection of cycles, while the `r label_metric("edge_flip")`r is more harsh for differences in the number of bifurcations (`r ref("fig", "topology_scores_overview", "c")`). The main difference however is that the `r label_metric("him")`r takes into account edge lengths when comparing two trajectories, as illustrated in (`r ref("fig", "topology_scores_overview", "d")`).

```{r, results = "asis"}
add_fig(
  result_file("topology_scores_overview.rds", experiment_id = "02-metric_characterisation/02-individual_metrics"),
  "topology_scores_overview",
  glue::glue("{label_metric('isomorphic')}, {label_metric('edge_flip')} and {label_metric('him')} ."),
  "",
  width = 12,
  height = 12
)
```

To summarise, the different topology based scores are useful for different scenarios:

- If the two trajectories should only be compared when the topology is exactly the same, the `r label_metric("isomorphic")` should be used.
- If it is important that the topologies are similar (but possible exactly isomorphic), the `r label_metric("edge_flip")` is most appropriate.
- If the topologies should be similar, but shorter edges should not be punished as hard as longer edges, the `r label_metric("HIM")` is most appropriate.

## `r label_metric("F1_branches")` and `r label_metric("F1_milestones")`: Comparing how well the cells are clustered in the trajectory

Perhaps one of the simplest ways to calculate the similarity between the cellular positions of two topologies is by mapping each cell to its closest milestone _or_ branch (`r ref("fig", "clustering_scores_overview")`). These clusters of cells can then be compared using one of the many external cluster evaluation measures [@saelensComprehensiveEvaluationModule2018]. When selecting such a metric, we had two conditions:

- Because we allow some methods to filter cells in the trajectory, the metric should be able to handle "non-exhaustive assignment", where some cells are not assigned to any cluster.
- The metric should give each cluster equal weight, so that rare cell stages are equally important as large stages.

Based on these requirements, and on the analysis in [@saelensComprehensiveEvaluationModule2018], we chose the $\textrm{F1}$ score between the $\textrm{Recovery}$ and $\textrm{Relevance}$. If $C$ and $C'$ are two cell clusters:

$Jaccard(c, c') = \frac{|c \cap c'|}{|c \cup c'|}$

$\textrm{Recovery} = \frac{1}{|C|} \sum_{c \in C}{\max_{c' \in C'}{\textrm{Jaccard(c, c')}}}$

$\textrm{Relevance} = \frac{1}{|C'|} \sum_{c' \in C'}{\max_{c \in C}{\textrm{Jaccard(c, c')}}}$

$textrm{F1} = \frac{2}{\frac{1}{\textrm{Recovery}} + \frac{1}{\textrm{Relevance}}}$

```{r, results = "asis"}
add_fig(
  result_file("clustering_scores_overview.rds", experiment = "02-metric_characterisation/02-individual_metrics"),
  "clustering_scores_overview",
  glue::glue("Mapping cells to their closest milestone or branch for the calculation of the {label_metric('F1_milestones')} and {label_metric('F1_branches')} ."),
  "",
  width = 12,
  height = 5
)
```

## `r label_metric("correlation")`: Correlation between geodesic distances

When the position of a cell is the same in both the gold standard and the prediction, its distances to all other cells in the trajectory should also be the same. This observation is the basis for the `r label_metric("correlation")` metric.

```{r, results = "asis"}
add_fig(
  result_file("metrics_geodesic.svg", "manual_figures"),
  "metrics_geodesic",
  "The calculation of geodesic distances on a small example trajectory.",
  "a) A toy example containing four milestones (W to Z) and five cells (a to e). b) The corresponding milestone network, milestone percentages and regions of delayed commitment, when the toy trajectory is converted to the common trajectory model. c) The calculations made for calculating the pairwise geodesic distances. d) A heatmap representation of the pairwise geodesic distances."
)
```

The geodesic distance is the distance a cell has to through the trajectory space to get from one position to another (`r ref("fig", "metrics_geodesic")`). The way this distance is calculated depends on how two cells are positioned:

- **Both cells are on the same edge in the milestone network.** In this case, the distance is defined as the product of the difference in milestone percentages and the length of the transition they both reside on. The geodesic distance is defined as the product of the difference in milestone percentages and the length of their common edge. For cells $a$ and $b$ in the example, $d(a, b)$ is equal to $1 \times (0.9 - 0.2) = 0.7$.
- **Cells reside on different edges in the milestone network.** First, the distance of the cell to all its nearby milestones is calculated, based on its percentage within the edge and the length of the edge. These distances in combination with the milestone network are used to calculate the shortest path distance between the two cells. For cells $a$ and $c$ in the example, $d(a, X) = 1 \times 0.9$ and $d(c, X) = 3 \times 0.2$, and therefore $d(a, c) = 1 \times 0.9 + 3 \times 0.2$. 

The geodesic distance can be easily extended towards cells within regions of delayed commitment. When both cells are part of the same region of delayed commitment, the geodesic distance was defined as the manhattan distances between the milestone percentages weighted by the lengths from the milestone network. For cells $d$ and $e$ in the example, $d(d, e)$ is equal to $0 \times (0.3 - 0.2) + 2 \times (0.7 - 0.2) + 3 \times(0.4 - 0.1)$, which is equal to $1.9$. The distance between two cells where one is part of a region of delayed commitment is calculated similarly to the previous paragraph, by first calculating the distance between the cells and their neighbouring milestones first, then calculating the shortest path distances between the two.

Calculating the pairwise distances between cells scales quadratically with the number of cells, and would therefore not be scaleable for large datasets. For this reason, a set of waypoint cells are defined *a priori*, and only the distances between the waypoint cells and all other cells is calculated, in order to calculate the correlation of geodesic distances of two trajectories (`r ref("fig", "waypoints_overview", "a")`). These cell waypoints are determined by viewing each milestone, edge between two milestones and region of delayed commitment as a collection of cells. We do stratified sampling from each collection of cells by weighing them by the total number of cells within that collection. For calculating the `r label_metric("correlation")` between two trajectories, the distances between all cells and the union of both waypoint sets is computed.

To select the number of cell waypoints, we need to find a trade-off between the accuracy versus the time to calculate `r label_metric("correlation")`. To select an optimal number of cell waypoints, we used the synthetic dataset with the most complex topology, and determined the `r label_metric("correlation")` at different levels of both cell shuffling and number of cell waypoints (`r ref("fig", "waypoints_overview", "b")`). We found that using cell waypoints does not induce a systematic bias in the `r label_metric("correlation")`, and that its variability was relatively minimal when compared to the variability between different levels of cell shuffling when using 100 or more cell waypoints.

```{r, results = "asis"}
add_fig(
  result_file("waypoints_overview.rds", experiment = "02-metric_characterisation/02-individual_metrics"),
  "waypoints_overview",
  glue::glue("Determination of cell waypoints"),
  glue::glue("a) Illustration of the stratified cell sampling using an example dataset (top). Each milestone, edge between two milestones and region of delayed commitment is seen as a collection of cells (middle), and the number of waypoints (100 in this case) are divided over each of these collection of cells (bottom). b) Accuracy versus time to calculate {label_metric('correlation')}"),
  width = 12, height = 8
)
```

## `r label_metric("rf_nmse")` and `r label_metric("lm_nmse")`: Using the positions of the cells within one trajectory to predict the cellular positions in the other trajectory

An alternative approach to detect whether the positions of cells are similar between two trajectories, is to use the positions of one trajectory to predict the positions within the other trajectory. If the prediction error for a particular cell is low, the higher the similarity between the cellular positions.

Specifically, we implemented two metrics which predict the milestone percentages from the gold standards by using the predicted milestone percentages as features  (`r ref("fig", "metrics_prediction")`). We did this with two regression methods, linear regression ($\textit{lm}$, using the R `lm` function) and Random Forest ($\textit{rf}$, implemented in the *ranger* package [@wright_rangerfastimplementation_2017]). In both cases, the accuracy of the prediction was measured using the Mean Squared error ($\mathit{MSE}$), in the case of Random forest we used the out-of-bag mean-squared error. Next, we calculated $\mathit{MSE}_{worst}$ equal to the $\mathit{MSE}$ when predicting all milestone percentages as the average. We used this to calculate the normalised mean squared error as $\mathit{NMSE} = 1 - \frac{\mathit{MSE}}{\mathit{MSE}_{worst}}$. We created a regression model for every milestone in the gold standard, and averaged the $\mathit{NMSE}$ values to finally obtain the `r label_metric("rf_nmse")` and `r label_metric("lm_nmse")` scores.

```{r, results = "asis"}
add_fig(
  result_file("metrics_prediction.svg", "manual_figures"),
  "metrics_prediction",
  glue::glue("The calculation of {label_metric('lm_nmse')} distances on a small example trajectory."),
  ""
)
```

# Application metrics

Although most metrics described above already assess some aspects directly relevant to the user, such as whether the method is good at finding the right topology, these metrics do not assess the quality of downstream analyses and hypotheses which can be generated from these models. 

## `r label_metric("featureimp_cor")`: The accuracy of dynamical differentially expressed features/genes.

Perhaps the main advantages of studying cellular dynamic processes using single-cell -omics data is that the dynamics of gene expression can be studied for the whole transcriptome. This can be used to construct other models such as dynamic regulatory networks and gene expression modules. Such analyses rely on a "good-enough" cellular ordering, so that it can be used to identify dynamical differentially expressed genes.

To calculate the `r label_metric("featureimp_cor")` we used Random forest regression to rank all the features according to their importance in predicting the positions of cells in the trajectory. Specifically, we calculated the geodesic distances for each cell to all milestones in the trajectory. Next, we trained a Random Forest regression model (implemented in the R _ranger_ package [@wright_rangerfastimplementation_2017], [https://github.com/imbs-hl/ranger](https://github.com/imbs-hl/ranger)) to predict these distances for each milestone, based on the expression of genes within each cell. We then extracted feature importances using the Mean Decrease in Impurity (`importance = 'impurity'` parameter of the `ranger` function), as illustrated in (`r ref("fig", "featureimp_overview")`). The overall importance of a feature (gene) was then equal to the mean importance over all milestones.

```{r, results = "asis"}
add_fig(
  result_file("featureimp_overview.rds", experiment = "02-metric_characterisation/02-individual_metrics"),
  "featureimp_overview",
  glue::glue(""),
  glue::glue(""),
  width = 12, height = 6
)
```

Random forest regression has two main hyperparameters. The number of trees to be fitted (`num_trees` parameter) was fixed to `10000` to provide accurate and stable estimates of the feature importance (`r ref("fig", "featureimp_cor_distributions")`). The number of features on which can be split (`mtry` parameter) was set to 1% of all available features (instead of the default square-root of the number of features), as to make sure that predictive but highly correlated feautres (omnipresent in transcriptomics data) are not suppressed in the ranking.

```{r, results = "asis"}
add_fig(
  result_file("feautreimp_cor_distributions.rds", experiment = "02-metric_characterisation/02-individual_metrics"),
  "feautreimp_cor_distributions",
  glue::glue(""),
  glue::glue(""),
  width = 12, height = 6
)
```

# Overall metrics

An overall metric should not only focus on 

# Metric conformity

```{r}
experiment("02-metric_characterisation/01-metric_conformity")

dataset_design <- read_rds(derived_file("dataset_design.rds"))
```

Although most metrics described in the previous section make sense intuitively, this does not necessarily mean that these metrics are robust and will generate reasonable results when used for benchmarking. This is because different methods and datasets will all lead to a varied set of trajectory models:

- Real datasets have all cells grouped onto milestones
- Some methods place all cells in a region of delayed commitment, others never generate a region of delayed commitment
- Some methods always return a linear trajectory, even if a bifurcation is present in the data
- Some methods filter cells

A good metric, especially a good overall metric, should work in all these circumstances. To test this, we designed a set of rules to which a good metric should conform, and assessed empirically whether a metric conforms to these rules.

We generated a panel of toy datasets (using our [_dyntoy_](https://github.com/dynverse/dyntoy) package, [https://github.com/dynverse/dyntoy](https://github.com/dynverse/dyntoy)) with all possible combinations of:

- \# cells: `r sort(unique(dataset_design$num_cells)) %>% label_vector()`
- \# features: 200
- topologies: linear, bifurcation, multifurcating, tree, cycle, connected graph and disconnected graph
- Whether cells are placed on the milestones (as in real data) or on the edges/regions of delayed commitment between the milestones (as in synthetic data)

We then perturbed the trajectories in these datasets in certain ways, and tested whether the scores follow an expected pattern. An overview of the conformity of every metric is first given in `r ref("table", "conformity_overview")`. The individual rules and metric behaviour are discussed more into detail after that.

```{r}
assessments <- read_rds(derived_file("assessments.rds"))
rules <- read_rds(derived_file("rules.rds"))
```

```{r, results="asis"}
table <- map(c("html", "latex", "markdown"), function(format) {
  table <- assessments %>%
    left_join(rules %>% select(id, name), by = c("rule_id" = "id")) %>%
    unnest(conformity) %>% 
    mutate(
      metric_id = label_metrics(metric_id) %>% as.character() %>% forcats::fct_inorder()
    )
  
  if (format != "markdown") {
    table <- table %>% 
      mutate(
        conforms = kableExtra::cell_spec(
          ifelse(conforms, "\U2714", "\U2716"),
          background = ifelse(conforms, "#2ECC40", "#FF4136"),
          format = format
        ),
        rule_id = forcats::fct_inorder(rule_id)
      ) 
  } else {
    table <- table %>% 
      mutate(
        conforms = ifelse(conforms, "\U2714", "\U2716")
      )
  }

  
  table <- table %>%
    spread(metric_id, conforms) %>%
    select(-rule_id)
  
  if (format != "markdown") {
    knitr::kable(table, format = format, escape = FALSE, booktabs = TRUE) %>% 
      kableExtra::kable_styling(full_width = FALSE) %>% 
      kableExtra::column_spec(1, width="15em") %>% 
      kableExtra::column_spec(2:ncol(table), width = "3em") %>% 
      kableExtra::row_spec(0, angle = 30) %>% 
      gsub("\\\\addlinespace", "", .)
  } else {
    knitr::kable(table, format = format)
  }
}) %>% set_names(c("html", "latex", "markdown"))

add_table(
  table,
  "conformity_overview",
  "Overview of whether a particular metric conforms to a particular rule"
)
```

\pagebreak

```{r results='asis'}
# assessment <- dynutils::extract_row_to_list(assessments, 1)

walkdf(assessments[1, ], function(assessment) {
  cat(knitr::knit_child(result_file("assessment.Rmd", experiment = "02-metric_characterisation/01-metric_conformity"), quiet=TRUE, envir = environment()))
})
```
